# PostgreSQL 18 Production Rules

## 1. Architecture & Schema Design

### Database Structure
- **MUST** normalize to at least 3NF to eliminate redundancy and maintain referential integrity
- **MUST** constrain ordinary users to user-private schemas by removing public CREATE privileges and creating schemas matching each username
- **NEVER** name objects with `pg_` prefix to prevent conflicts with system objects
- **ALWAYS** use lowercase naming for tables, columns, and constraints (PostgreSQL folds unquoted identifiers to lowercase)
- **MUST** give constraints explicit names for clarity in error messages and DDL operations (pattern: `{tablename}_{columnname}_key`)
- **MUST** use schemas to organize objects logically and separate third-party application objects

### Normalization vs Denormalization
- **MUST** start with normalized design; only denormalize after identifying specific performance bottlenecks
- **ALWAYS** extract JSONB keys into proper columns when they share the same keys with consistent data types and are used in WHERE clauses or joins
- **NEVER** rely on JSONB for columns requiring foreign key constraints or statistical query planning
- **MUST** use materialized views instead of denormalization for aggregated data and reporting queries
- **ALWAYS** store literals (not IDs) in materialized views to avoid runtime joins

### Partitioning Strategies
- **MUST** use declarative partitioning over inheritance-based partitioning (deprecated)
- **ALWAYS** choose partition columns based on WHERE clause usage patterns, not arbitrary fields
- **MUST** use RANGE partitioning for time-series data, sequential IDs, and when archiving old data (dropping entire partitions)
- **MUST** use HASH partitioning for high-cardinality columns (user IDs, transaction IDs) requiring even distribution across partitions
- **MUST** use LIST partitioning for discrete, predefined categorical values (country, department, status)
- **NEVER** create excessive partitions; aim for balanced partition count based on table size exceeding physical memory
- **MUST** include partition key columns in all PRIMARY KEY and UNIQUE constraints (architectural limitation)
- **ALWAYS** create indexes on partition key columns for optimal partition pruning

---

## 2. Data Types & Column Design

### Primary Key Strategy
- **NEVER** use SERIAL or BIGSERIAL; use IDENTITY columns instead (modern PostgreSQL standard)
- **MUST** use BIGINT for all primary key columns to prevent exhaustion (supports 2^63 values)
- **ALWAYS** use `GENERATED ALWAYS AS IDENTITY` over `GENERATED BY DEFAULT` to prevent manual insertion conflicts
- **ALWAYS** use UUID v7 (PostgreSQL 18+) when distributed/sharded systems require decentralized ID generation
- **NEVER** use random UUIDs for large datasets due to massive WAL bloat and insert performance degradation
- **MUST** use IDENTITY/SERIAL for single-database systems unless you need to hide meta-information (row counts)

### Text and Character Types
- **ALWAYS** use TEXT for variable-length strings without specific length requirements
- **NEVER** use VARCHAR without length constraints; it performs identically to TEXT
- **MUST** use VARCHAR(n) only when you have a legitimate business requirement for length validation
- **NEVER** use CHAR for variable-length data; it wastes space with padding

### Timestamp and Timezone Handling
- **NEVER** use `timestamp without time zone` or `time without time zone`
- **ALWAYS** use `timestamptz` for storing dates and times (stores as UTC, highly optimized, same compression as integers)
- **MUST** set database timezone to UTC for consistency
- **ALWAYS** use timezone names (not abbreviations) to handle daylight saving transitions correctly
- **NEVER** use `timetz` type; use `timestamptz` instead

### JSON Data Types
- **ALWAYS** use JSONB over JSON for production workloads that involve querying or manipulation
- **NEVER** use JSON type unless you need exact preservation of input text formatting or whitespace
- **MUST** create GIN indexes on JSONB columns when performing frequent key/value searches
- **NEVER** rely on JSONB for columns requiring foreign key constraints or statistical query planning

### Numeric Types
- **ALWAYS** use INTEGER for most whole number use cases (best balance of range/performance)
- **MUST** use NUMERIC/DECIMAL for monetary values requiring exact precision
- **NEVER** use floating-point types (REAL/DOUBLE) for financial calculations
- **NEVER** use money type; use NUMERIC or integer types for monetary values to prevent precision errors
- **ALWAYS** use BIGINT when values may exceed 2 billion

### Array and Complex Types
- **NEVER** use arrays as a substitute for proper relational design with foreign keys
- **MUST** use arrays only for tightly coupled one-to-many data that won't need independent querying
- **NEVER** iterate through PostgreSQL arrays by position; use `unnest()` to avoid O(N²) performance
- **ALWAYS** add GIN indexes on array columns when performing element searches
- **ALWAYS** use ENUM only for small, fixed sets of values (< 20 items)
- **NEVER** use ENUM for frequently changing value sets; use lookup tables with foreign keys instead
- **MUST** use DOMAINs to centralize validation logic across multiple tables/columns

---

## 3. Indexing & Query Optimization

### Index Strategy
- **ALWAYS** use B-tree indexes (default) for equality, range queries, and sorting on columns with high cardinality
- **MUST** use GIN indexes for JSONB columns, arrays, full-text search, and multi-value element queries
- **MUST** use BRIN indexes for large, naturally ordered tables (timestamps, sequential IDs) with linear sort correlation
- **MUST** use GiST indexes for geometric data types, spatial queries, and nearest-neighbor searches
- **NEVER** over-index; monitor `pg_stat_user_indexes` and drop unused indexes that slow writes
- **ALWAYS** create indexes after bulk inserts, then drop and recreate GIN indexes for large bulk operations
- **MUST** index all foreign key columns to prevent sequential scans on joins and cascading operations
- **ALWAYS** create partial indexes when repeatedly querying a small, well-defined subset of rows
- **NEVER** create indexes on columns with low cardinality unless using a partial index for selective queries
- **ALWAYS** use covering indexes (INCLUDE clause) to enable index-only scans for frequently accessed column combinations
- **MUST** ensure tables are regularly VACUUMed for index-only scans to work efficiently (visibility map dependency)

### Query Performance Configuration
- **ALWAYS** set `random_page_cost` to 1.0-1.1 for SSD storage to encourage appropriate index usage
- **ALWAYS** set `effective_cache_size` to 50-75% of system RAM to help planner make accurate cost estimates
- **MUST** adjust `work_mem` per workload type: higher for analytics (64-256MB), lower for OLTP (4-16MB)
- **MUST** increase `default_statistics_target` from 100 to 500-1000 for columns with irregular distributions used in WHERE clauses
- **ALWAYS** increase `work_mem` to enable in-memory hash joins instead of disk-based operations

### Query Optimization Patterns
- **NEVER** use `SELECT *` statements; explicitly select only required columns
- **NEVER** use `NOT IN (SELECT ...)` - it doesn't optimize to anti-join and causes O(N²) performance; use `NOT EXISTS` instead
- **NEVER** use `DISTINCT` to fix unexpected duplicates - indicates a query logic problem requiring proper joins
- **ALWAYS** use prepared statements for repeatedly executed queries to save planning overhead
- **ALWAYS** use explicit JOIN syntax instead of WHERE clause joins - helps query planner optimization
- **NEVER** apply functions to indexed columns in WHERE clauses - prevents index usage; create functional indexes instead

### EXPLAIN Analysis
- **ALWAYS** use `EXPLAIN (ANALYZE, BUFFERS)` to identify performance bottlenecks
- **ALWAYS** compare estimated rows vs actual rows - large discrepancies indicate stale statistics requiring ANALYZE
- **ALWAYS** look for "Rows Removed by Filter" - high values indicate missing indexes
- **ALWAYS** run ANALYZE after significant data changes (>5-10% of rows modified) to update query planner statistics
- **NEVER** extrapolate EXPLAIN results from small datasets to production-scale data

### Join Optimization
- **ALWAYS** ensure accurate table statistics via regular ANALYZE - incorrect row estimates lead to suboptimal join strategies
- **NEVER** join more than 5 tables simultaneously - each join exponentially increases plan complexity
- **ALWAYS** join smaller tables first when possible - optimizer uses this for nested loop join decisions
- **MUST** index foreign key columns used in JOIN conditions to enable efficient nested loop joins
- **NEVER** disable join types (enable_nestloop, enable_hashjoin, enable_mergejoin) in production

### CTEs and Aggregation
- **ALWAYS** use `NOT MATERIALIZED` CTEs when you want predicate pushdown and index usage on referenced tables
- **ALWAYS** use `MATERIALIZED` CTEs when the subquery is expensive and referenced multiple times to avoid duplicate computation
- **ALWAYS** prefer subqueries over CTEs for single-use, simple queries - allows better query optimization
- **ALWAYS** place columns with higher cardinality first in GROUP BY clauses for more efficient hash aggregation
- **MUST** use GROUP BY instead of DISTINCT when possible for complex queries - GROUP BY can use parallel query execution
- **NEVER** use COUNT(DISTINCT) on large datasets without indexes - create partial indexes or use approximate methods

---

## 4. Data Integrity & Constraints

### Constraint Fundamentals
- **MUST** define PRIMARY KEY on every table (though PostgreSQL doesn't enforce this, best practice dictates it)
- **ALWAYS** use NOT NULL constraints instead of CHECK constraints for null validation (more efficient)
- **MUST** use CHECK constraints to enforce complex multi-column data validation rules at database level
- **ALWAYS** name your constraints explicitly - this makes error messages informative and debugging simpler
- **MUST** design constraints to allow null values, then apply column NOT NULL constraints separately

### Foreign Key Constraints
- **ALWAYS** create indexes on foreign key columns in child tables for join and cascade performance
- **ALWAYS** index both ends of foreign key relationships - not just the primary key but also the reference column
- **MUST** carefully consider ON DELETE and ON UPDATE actions - CASCADE is convenient but can lead to unexpected data loss
- **NEVER** use CASCADE actions without understanding full impact - prefer RESTRICT or SET NULL for safer referential integrity
- **MUST** use NOT VALID when adding foreign keys to large existing tables - commits immediately without locking, then validate separately
- **MUST** temporarily remove foreign keys during bulk operations, then re-add afterward

### Unique and Check Constraints
- **ALWAYS** use UNIQUE B-tree indexes to enforce uniqueness (only index type supporting unique constraints)
- **MUST** use INSERT ... ON CONFLICT DO NOTHING when relying on constraint violations for logic
- **NEVER** rely on catching constraint violations for normal application flow without using ON CONFLICT
- **NEVER** reference other table data in check constraints - PostgreSQL cannot guarantee constraint validity after subsequent changes
- **ALWAYS** ensure check constraint expressions will not throw errors - PostgreSQL assumes they are immutable

### Advanced Constraint Techniques
- **MUST** make constraints DEFERRABLE when implementing circular foreign key dependencies or complex multi-table operations
- **NEVER** use triggers for simple data validation; use CHECK constraints or NOT NULL instead
- **ALWAYS** use UNIQUE constraints with `NULLS NOT DISTINCT` when nulls should be considered equal (PostgreSQL 15+)
- **MUST** use validation as an ongoing process requiring continuous effort - data integrity isn't a one-time setup
- **MUST** implement multi-layer validation - application-level validation as first defense, database constraints as second line

---

## 5. Connection Management & Transactions

### Connection Pooling
- **MUST** use PgBouncer 1.21+ with transaction mode pooling and `max_prepared_statements > 0` for prepared statement support
- **MUST** use transaction pooling mode unless requiring session-level features (advisory locks, LISTEN/NOTIFY)
- **NEVER** use statement pooling mode for production workloads (breaks multi-statement transactions)
- **MUST** calculate max_connections as `(CPU cores * 2) + effective_spindle_count`, excluding hyperthreaded cores
- **MUST** start pool size at ~50% of max_connections, then tune based on CPU utilization
- **NEVER** exceed 15 connections per CPU core when setting `max_connections`
- **MUST** set `max_prepared_statements=1000` (or higher than commonly used prepared statements) for optimal performance

### Transaction Isolation
- **ALWAYS** use READ COMMITTED (default) for most OLTP workloads with high concurrency
- **MUST** use REPEATABLE READ for financial transactions requiring consistent snapshots across multiple queries
- **MUST** use SERIALIZABLE only when absolute consistency is required; implement retry logic for serialization failures
- **NEVER** use SERIALIZABLE without application-level transaction retry mechanisms
- **ALWAYS** prepare applications to handle serialization failures by aborting and retrying entire transactions
- **MUST** retry entire transaction from scratch, not from savepoint - PostgreSQL snapshots make savepoint retries fail identically

### Deadlock Prevention
- **MUST** acquire locks on multiple objects in consistent order across all transactions - ordering prevents most deadlocks
- **MUST** keep transactions as short as possible to minimize lock hold duration and conflict probability
- **NEVER** perform external API calls, file I/O, or expensive computations inside transactions
- **MUST** use `SELECT ... FOR UPDATE SKIP LOCKED` to avoid deadlocks in queue-like workloads
- **MUST** set deadlock_timeout to exceed typical transaction duration (default 1s is minimum, increase for heavily loaded servers)
- **NEVER** set deadlock_timeout above 10 seconds - delays error reporting without preventing deadlocks

### Timeout Configuration
- **MUST** set `idle_in_transaction_session_timeout` to prevent bloat from abandoned transactions (recommended: 5 minutes/300000ms)
- **NEVER** set statement_timeout globally in postgresql.conf - configure per-role, per-database, or per-session with SET LOCAL
- **MUST** set statement_timeout to prevent runaway queries from consuming resources indefinitely
- **NEVER** disable idle_in_transaction_session_timeout in production - idle transactions block VACUUM and cause bloat

### Long-Running Transactions
- **MUST** split long-running operations into smaller committed chunks when ACID properties aren't required for entire operation
- **NEVER** leave transactions idle - commit or rollback immediately when application logic completes
- **MUST** monitor pg_stat_activity for transactions in "idle in transaction" state as production red flag
- **ALWAYS** close connections properly in application finally/defer blocks to prevent connection and transaction leaks

---

## 6. Security & Access Control

### Authentication
- **NEVER** use `trust` authentication method in `pg_hba.conf`, especially for remote connections or production environments
- **MUST** use SCRAM-SHA-256 for password hashing instead of MD5
- **MUST** change the default `postgres` superuser password immediately after installation
- **NEVER** use `0.0.0.0/0` netmask in `pg_hba.conf` unless absolutely necessary; restrict to specific IP ranges
- **MUST** use `hostssl` entries in `pg_hba.conf` for remote connections to enforce TLS encryption
- **NEVER** allow superuser connections from remote hosts; restrict superuser access to local connections only
- **MUST** document every line in `pg_hba.conf` explaining why access is needed and who requested it

### Role Management
- **MUST** grant only minimum necessary privileges for each role to perform their specific tasks
- **NEVER** use the `postgres` superuser role for application connections
- **MUST** revoke unnecessary privileges from the `public` schema
- **NEVER** create `pg_hba.conf` entries that allow connections to all databases or all users
- **MUST** create unique roles for each user and application; avoid shared accounts
- **MUST** regularly audit roles and permissions to align with current organizational needs

### Row-Level Security (RLS)
- **MUST** enable Row-Level Security (RLS) on all tables in exposed schemas containing tenant or user data
- **ALWAYS** use `FORCE ROW LEVEL SECURITY` to prevent superuser bypass unless explicitly intended
- **MUST** create RLS policies using STABLE functions without row data parameters (allows Postgres to cache results)
- **ALWAYS** add indexes on columns used in RLS policy WHERE clauses to prevent performance degradation
- **NEVER** store authorization data in `raw_user_meta_data` (user-modifiable); use `raw_app_meta_data` instead
- **MUST** treat RLS policies as critical schema objects in version control and documentation

### Encryption
- **MUST** set `ssl = on` in `postgresql.conf` to enable TLS/SSL
- **MUST** use TLS 1.2 or higher; disable older SSL/TLS versions
- **MUST** use `sslmode=verify-full` on client connections to verify server certificate and hostname
- **MUST** configure `pg_hba.conf` with `hostssl` to enforce encrypted connections; using `host` alone permits unencrypted fallback
- **MUST** verify active SSL connections using queries against `pg_stat_ssl` joined with `pg_stat_activity`

### Audit Logging
- **MUST** install and enable `pgaudit` extension for compliance and security auditing
- **MUST** configure `pgaudit.log` parameter based on specific audit requirements (avoid blanket `ALL` in production)
- **MUST** export PostgreSQL logs to centralized logging systems (ELK, Splunk, etc.)
- **MUST** regularly review audit logs for suspicious activity and failed authentication attempts
- **MUST** carefully scope `pgaudit.log` settings to avoid log explosion that impacts performance

### Network Hardening
- **MUST** set `listen_addresses` in `postgresql.conf` to specific IP addresses, not `*`
- **NEVER** expose PostgreSQL directly to the public internet
- **MUST** use firewall rules or security groups to restrict access to trusted IP ranges only

---

## 7. Application Integration

### SQL Injection Prevention
- **ALWAYS** use parameterized queries (prepared statements) with variable binding, never string concatenation
- **NEVER** trust ORM abstraction alone; verify generated queries don't bypass parameterization
- **MUST** update to PostgreSQL 17.3+, 16.7+, 15.11+, 14.16+, or 13.19+ to patch CVE-2025-1094 escaping vulnerabilities
- **NEVER** use dynamic SQL construction in stored procedures without proper quoting functions
- **MUST** validate that ORMs use prepared statements for all queries, especially in framework upgrades

### N+1 Query Prevention
- **MUST** use eager loading (select_related, prefetch_related, Include, EntityGraph) instead of default lazy loading
- **MUST** enable query logging during development to detect N+1 patterns
- **NEVER** load relationships in loops; use JOIN operations or batch fetching
- **MUST** use `EXPLAIN ANALYZE` to verify query patterns before deploying ORM code
- **MUST** leverage PostgreSQL CTEs to solve complex N+1 scenarios when ORM patterns fail

### Bulk Operations
- **MUST** use the `COPY` command for loading more than 1,000 rows
- **MUST** create indexes after bulk loading data, not before
- **NEVER** use individual INSERT statements for bulk operations; use batched inserts at minimum
- **MUST** disable or drop indexes before bulk operations exceeding 10% of table size, then recreate them
- **MUST** run `ANALYZE` immediately after bulk operations complete
- **MUST** temporarily increase `max_wal_size` during large bulk loads to reduce checkpoint frequency

### Error Handling and Retries
- **MUST** implement retry logic for serialization failures (SQLSTATE 40001) when using Repeatable Read or Serializable isolation
- **MUST** retry deadlock failures (SQLSTATE 40P01) with exponential backoff
- **NEVER** assume PostgreSQL will automatically retry failed transactions
- **MUST** retry complete transactions including all decision logic, not just failed queries
- **ALWAYS** use `ROLLBACK` in error handlers to prevent "idle in transaction" states
- **MUST** configure application timeouts shorter than `statement_timeout` to prevent hung connections

### Schema Migrations
- **MUST** version control all database schema changes alongside application code
- **MUST** use migration-based tools (Flyway, Liquibase, Alembic) rather than manual schema changes
- **MUST** use the expand-and-contract pattern for zero-downtime schema migrations
- **NEVER** skip migration steps or apply migrations out of order
- **MUST** test migrations with rollback capabilities before production deployment
- **NEVER** modify existing migration files after they've been applied to production

### Testing Strategies
- **MUST** wrap each test in a transaction and use `ROLLBACK` to restore state
- **MUST** test with production-scale data volumes; never rely solely on small test datasets
- **MUST** run `ANALYZE` before testing query performance
- **NEVER** test with data distributions that don't match production
- **MUST** use separate test databases with identical schema versions to production

### Application vs Database Logic
- **MUST** implement data integrity constraints (foreign keys, check constraints) in the database, not just application code
- **MUST** use database-side logic for purely data-driven operations requiring transactional consistency
- **MUST** use application-side logic for business rules involving external APIs, ML models, or distributed services
- **NEVER** duplicate complex business logic in both stored procedures and application code
- **ALWAYS** prefer application-side logic when horizontal scalability is required

---

## 8. Backup, High Availability & Disaster Recovery

### Backup Strategies
- **MUST** use pg_basebackup for databases >100-200GB; pg_dump becomes too slow for production recovery windows
- **MUST** combine physical backups (pg_basebackup) with continuous WAL archiving for point-in-time recovery capability
- **MUST** test restore procedures regularly in non-production environments; untested backups are not backups
- **MUST** configure `wal_level = replica` minimum and `archive_mode = on` for any production system requiring PITR
- **NEVER** rely solely on pg_dump for disaster recovery in production; it lacks PITR support and is too slow for large databases
- **MUST** take full base backups at minimum weekly, with continuous WAL archiving between full backups
- **MUST** verify backup integrity; corrupted backups discovered during disaster recovery are worthless
- **ALWAYS** store backups off-site from primary database location

### Point-in-Time Recovery (PITR)
- **MUST** implement WAL archiving to achieve recovery to any point in time since your base backup
- **MUST** configure `archive_command` to copy WAL files to reliable, monitored storage before PostgreSQL recycles them
- **MUST** monitor WAL archiving failures; failed archiving blocks WAL recycling and will fill disk
- **ALWAYS** validate that archived WAL files can be retrieved and are not corrupted

### Replication
- **MUST** use synchronous replication when RPO (Recovery Point Objective) is zero; asynchronous replication risks data loss
- **MUST** configure `synchronous_standby_names` when using synchronous replication to specify which standbys must confirm writes
- **MUST** use `synchronous_commit = remote_apply` for strictest consistency guarantees in synchronous mode
- **NEVER** use two-node synchronous replication for critical systems; standby failure creates indefinite waiting
- **MUST** enable logical replication slot synchronization (`failover = true`, `sync_replication_slots`) in PostgreSQL 18 for seamless logical replication across physical failover
- **ALWAYS** verify replication lag stays within acceptable bounds for your RTO requirements

### Failover Automation
- **MUST** implement automated failover tooling (Patroni, repmgr, pg_auto_failover, EFM) for production systems; manual failover is too slow
- **MUST** test failover procedures regularly under controlled conditions
- **MUST** configure health checks that accurately detect primary failure without false positives
- **NEVER** assume failover will be instantaneous; budget for RTO of 15-30 seconds minimum even with automation

### Monitoring and Alerting
- **MUST** monitor replication lag on all standbys; excessive lag means potential data loss during failover
- **MUST** alert at 70-80% CPU threshold to enable proactive response before resource exhaustion
- **MUST** alert at 80% of max connections for ≥30 minutes to prevent connection exhaustion
- **MUST** monitor buffer cache hit ratio; low ratios indicate inefficient memory usage
- **MUST** monitor deadlocks and lock contention that cause query failures
- **MUST** track table/index bloat to prevent unnecessary disk consumption
- **MUST** monitor WAL archiving lag and failures; archiving failures will fill disks
- **NEVER** rely on email-only alerts for critical conditions; alerts get lost in noise

### Upgrade Strategies
- **MUST** use logical replication for zero-downtime upgrades between major versions when downtime cannot be tolerated
- **MUST** perform multiple dry runs in staging environments before production upgrades
- **MUST** use pg_upgrade with `--link` mode for huge multi-terabyte databases when brief downtime (minutes) is acceptable
- **MUST** have tested rollback strategy before starting any production upgrade
- **NEVER** upgrade production without verifying application compatibility with new PostgreSQL version in test environment

---

## 9. Maintenance & Operations

### Vacuum and Bloat Management
- **MUST** rely on properly configured autovacuum; never disable autovacuum daemon
- **ALWAYS** tune `autovacuum_vacuum_scale_factor` (default 0.2) and `autovacuum_vacuum_threshold` (default 50) for workload
- **NEVER** run VACUUM FULL on production tables during normal operations; use only for extreme bloat during maintenance windows
- **MUST** run manual ANALYZE after bulk inserts or significant schema changes affecting >5% of rows
- **ALWAYS** monitor table bloat using `pg_stat_user_tables` and `pgstattuple` extension
- **NEVER** perform manual VACUUM on entire database; target specific tables with identified bloat issues
- **MUST** run VACUUM regularly to update visibility maps - required for efficient index-only scans

### Performance Tuning
- **MUST** configure `shared_buffers` to 25% of system RAM
- **MUST** set `effective_cache_size` to 50-75% of total RAM
- **ALWAYS** set `random_page_cost` appropriately: 4.0 for spinning disks, 1.0-1.1 for SSDs
- **NEVER** set `max_connections` excessively high - each connection consumes work_mem and causes context switching overhead
- **ALWAYS** leverage PostgreSQL 18's new asynchronous I/O subsystem for 2-3× faster sequential scans on large tables

### Extension Management
- **MUST** install extensions in dedicated schemas (not public) for better organization and security
- **ALWAYS** use pgvector with HNSW indexes and `vector_cosine_ops` operator class for similarity search optimization
- **MUST** use TimescaleDB hypertables for time-series data (partitions by time automatically with superior query optimization)
- **ALWAYS** use PostGIS for geospatial data requiring coordinate storage and spatial queries
- **NEVER** enable extensions without understanding security implications; review extension code for sensitive databases

### Statistics and Analysis
- **ALWAYS** enable and properly configure autovacuum - it's critical for query performance and prevents table bloat
- **MUST** increase autovacuum aggressiveness for high-write tables by lowering `autovacuum_vacuum_scale_factor` from default 0.2
- **MUST** monitor `pg_stat_user_indexes` regularly to identify unused indexes (zero scans)
- **ALWAYS** drop unused indexes that appear in `pg_stat_user_indexes` with zero scans
- **MUST** monitor pg_stat_activity regularly to identify stuck connections, long-running queries, and idle transactions

---

## 10. PostgreSQL 18 Specific Features

### New Capabilities
- **ALWAYS** leverage PostgreSQL 18's new asynchronous I/O subsystem for 2-3× faster sequential scans on large tables
- **ALWAYS** use PostgreSQL 18's enhanced EXPLAIN output showing WAL writes and CPU time for comprehensive analysis
- **MUST** test queries with the new AIO subsystem to validate expected performance improvements on I/O-bound workloads
- **ALWAYS** use UUID v7 (PostgreSQL 18+) when distributed/sharded systems require decentralized ID generation
- **MUST** enable logical replication slot synchronization (`failover = true`, `sync_replication_slots`) in PostgreSQL 18 for seamless logical replication across physical failover
- **MUST** configure `synchronized_standby_slots` to ensure logical slots don't consume changes until physical standbys receive them
- **MUST** enable virtual generated columns (default in PostgreSQL 18) for computed values

---

These rules are derived from PostgreSQL 18 official documentation (released September 2025), recognized PostgreSQL experts (Cybertec, EDB, Crunchy Data, Percona), cloud provider best practices (AWS, Azure, Google Cloud), ORM maintainers, and security researchers. Following these rules ensures TOP QUALITY, PRODUCTION-READY PostgreSQL code.
